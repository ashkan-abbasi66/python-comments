{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset (http://yann.lecun.com/exdb/mnist/) includes 70K handwritten digits (Hello world of machine learning!).<br>\n",
    "<br>\n",
    "Each `28*28` image is **flattened** as a 784 **feature vector**. <br>\n",
    "Each feature represents one pixel's intensity (from 0 to 255).<br>\n",
    "\n",
    "It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]  # deprecated\n",
    "\n",
    "shuffle_index = np.random.permutation(X.shape[0])\n",
    "X =  X[shuffle_index,:]\n",
    "y = y[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import fetch_openml\n",
    "\n",
    "## Load data from https://www.openml.org/d/554\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html\n",
    "#X, y = fetch_openml('mnist_784', version=1, return_X_y=True, data_home = '../DATASETS') # it takes time! - Downloads everytime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255\n",
      "<class 'numpy.ndarray'>\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "ii = 69999\n",
    "sample = X[ii]\n",
    "sample_img = sample.reshape(28, 28)\n",
    "\n",
    "print(sample_img.min(), sample_img.max())\n",
    "print(type(sample_img))\n",
    "print(sample_img.dtype)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sample_img, \n",
    "           cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "print('label: %s'%y[ii]) # Note that the labels are stored as string!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60000\n",
    "\n",
    "X_train = X[:N]\n",
    "y_train = y[:N]\n",
    "\n",
    "X_test = X[N:]\n",
    "y_test = y[N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8., 9., 9., 3., 8., 1., 5., 0., 6., 8.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] <class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_test),type(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert class names into integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = int(y_test) # ERROR: only size-1 arrays can be converted to Python scalars\n",
    "\n",
    "\n",
    "def to_int(x): # scalar\n",
    "    return int(x)\n",
    "\n",
    "# ~ 1\n",
    "y_test = np.array(list(map(to_int,y_test)))\n",
    "\n",
    "# ~ 2\n",
    "# to_int_vec = np.vectorize(to_int)\n",
    "# y_test = to_int_vec(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] <class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_test),type(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(list(map(to_int,y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a 5-detector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_5[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifier - Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 0.58 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "per_clf = Perceptron(tol=1e-3, random_state=0)\n",
    "# tol, 1e-3: The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol)\n",
    "# shuffle, True: data is shuffled after each epoch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "per_clf.fit(X_train, y_train_5)\n",
    "\n",
    "print(\"Elapsed time %.2f seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9332166666666667"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf.score(X_train,y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems not bad! 93 percent **accuracy** on the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`score` computes `accuracy` when the estimator is a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9332166666666667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_train_5, per_clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per_clf.get_params(); per_clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf.score(X_test,y_test_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumb classifier!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90945"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class dumbClassifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "    \n",
    "dumb_clf = dumbClassifier()\n",
    "\n",
    "dumb_clf.fit(X_train, y_train_5)\n",
    "\n",
    "accuracy_score(y_train_5, dumb_clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90 percent accuracy for the dumb classifier!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`accuracy` is a common metric to evaluate classifiers. However, it is not good when the dataset is **skewed**/**imbalanced**. In this case, the accuracy is high but the overall performance is low. This is known as **accuracy paradox**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non-5 samples: 54567\n",
      "number of training samples: 60000\n",
      "0.90945\n"
     ]
    }
   ],
   "source": [
    "num_non_5 = np.sum((y_train_5 == False))\n",
    "N = len(y_train_5)\n",
    "\n",
    "print('number of non-5 samples:', num_non_5)\n",
    "print('number of training samples:', N)\n",
    "print(num_non_5/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_5, dumb_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifier - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 4.70 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# See here:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "# Gradient descent is based on the first order derivarive.\n",
    "log_clf = LogisticRegression(random_state=0, penalty = 'l2', solver = 'lbfgs') # second order derivative\n",
    "# penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’\n",
    "# solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "# C, default=1.0 -- Inverse of regularization strength; must be a positive float. \n",
    "# Like in support vector machines, smaller values specify stronger regularization.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "log_clf.fit(X_train, y_train_5)\n",
    "\n",
    "print(\"Elapsed time %.2f seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9781333333333333"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf.score(X_train,y_train_5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9758"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf.score(X_test,y_test_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   14,    27,    69, ..., 69974, 69975, 69986], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "ind_5 = np.where(y == 5)\n",
    "print(ind_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ii = 69974\n",
    "sample = X[ii]\n",
    "sample_img = sample.reshape(28, 28)\n",
    "\n",
    "plt.imshow(sample_img, \n",
    "           cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "print('label: %s'%y[ii]) # Note that the labels are stored as string!\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(784, 1)\n",
      "Predicted label:  [ True]\n",
      "Probabilities for each class [-,+]: [[0.01355969 0.98644031]]\n"
     ]
    }
   ],
   "source": [
    "print(sample.shape)\n",
    "sample_vec = np.c_[sample]\n",
    "print(sample_vec.shape)\n",
    "\n",
    "print('Predicted label: ',log_clf.predict(sample_vec.T))\n",
    "print('Probabilities for each class [-,+]:', log_clf.predict_proba(sample_vec.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifier - SGD Classifier\n",
    "\n",
    "SGD classifier is one of the mostly used in SKLearn\n",
    "\n",
    "BIG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised problems: (x,y)\n",
    "- classification\n",
    "- regression\n",
    "\n",
    "\n",
    "Unsupervised problems:\n",
    "- Clustering\n",
    "- Dimensionality Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashkan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 0.71 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# See here:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=0, loss = 'log')\n",
    "# loss, default=’hinge’ = Support Vector Machine\n",
    "#    The possible options are ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, \n",
    "#    ‘perceptron’, or a regression loss: ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, \n",
    "#    or ‘squared_epsilon_insensitive’.\n",
    "#\n",
    "# penalty{‘l1’, ‘l2’, ‘elasticnet’}, default=’l2’\n",
    "# alpha, default=0.0001\n",
    "# shuffle, default=True\n",
    "# verbose, default=0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "\n",
    "print(\"Elapsed time %.2f seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96605"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.score(X_train,y_train_5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.score(X_test,y_test_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here:<br>\n",
    "https://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use\n",
    "\n",
    ">Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "model = Pipeline([('scaler', StandardScaler()), \n",
    "                  ('sgd_clf', SGDClassifier(random_state=0, loss = 'log',alpha = 0.00001))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashkan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "c:\\users\\ashkan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 1.74 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(X_train, y_train_5)\n",
    "\n",
    "print(\"Elapsed time %.2f seconds.\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashkan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9726666666666667"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train,y_train_5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ashkan\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9713"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test_5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on the StandardScalar\n",
    "\n",
    "> Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "\n",
    ">For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. **If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
