{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa8b5bc",
   "metadata": {},
   "source": [
    "# Datasets in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57d2e3",
   "metadata": {},
   "source": [
    "Objective: decouple code for train and loading/preprocessing data\n",
    "\n",
    "data primitivis:\n",
    "- `torch.utils.data.Dataset`: stores samples and their corresponding labels / target\n",
    "- `torch.utils.data.DataLoader`: wraps an iterable around the dataset. Ease of access to samples.\n",
    "\n",
    "download and work with famous dataset: Image Datasets, Text Datasets, Audio\n",
    "\n",
    "Example: \n",
    "- `torchvision.datasets.MNIST`\n",
    "- `torchtext.datasets.IMDB`\n",
    "- `torchaudio.datasets.SPEECHCOMMANDS`\n",
    "\n",
    "All of them are inherited from `torch.utils.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a32c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.ToTensor() # PIL image ===> [0, 1] Pytorch Tensor\n",
    "\n",
    "trainset = datasets.MNIST(root='~/.pytorch/MNIST_data/', download=True, train = True, transform=transform) # C:\\Users\\ashkan\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aabb4b",
   "metadata": {},
   "source": [
    "## The content of `torch.utils.data.Dataset`'s object can be accessed by indexing\n",
    "\n",
    "All datasets implement `__getitem__` and `__len__`\n",
    "\n",
    "```python\n",
    "class CustomImageDataset(torch.utils.nn.Dataset):\n",
    "    def __init__(self,...):\n",
    "        super.__init__()\n",
    "        ...\n",
    "    def __getitem__:\n",
    "        ...\n",
    "    def __len__:\n",
    "        ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19505f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "3\n",
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "# image, label = trainset[50000] \n",
    "image, label = trainset.__getitem__(50000)\n",
    "\n",
    "print(type(image)) # 0 - 255\n",
    "print(label)\n",
    "\n",
    "print(len(trainset))\n",
    "print(trainset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b47382ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7fc54",
   "metadata": {},
   "source": [
    "shape is `Channel*Height*Width`\n",
    "\n",
    "`255*255*3` ===> `3*255*255`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7c89d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c16c9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.getextrema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451608b3",
   "metadata": {},
   "source": [
    "## `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ced8b",
   "metadata": {},
   "source": [
    "`DataLoader` wraps an iterator around `Dataset`\n",
    "\n",
    "- `__iter__`: returns the iterator object itself\n",
    "- `__next__`: returns the next item in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "404b85fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader) \n",
    "\n",
    "images, labels = dataiter.next() # next(dataiter)\n",
    "\n",
    "print(images.shape) # PyTorch: NCHW ----- Tensorflow/Keras: NHWC \n",
    "print(type(images))\n",
    "\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa8fd8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c23a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
