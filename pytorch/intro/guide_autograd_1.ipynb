{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816b48aa",
   "metadata": {},
   "source": [
    "# `autograd` introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b10ed9",
   "metadata": {},
   "source": [
    "<img src='assets/computational_graph_example_pytorch.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe24af3",
   "metadata": {},
   "source": [
    "Torch provides a module, `autograd`, for automatically calculating the gradients of tensors. We can use `autograd` to calculate the gradients of all our parameters with respect to the loss. \n",
    "\n",
    "`autograd` works by *keeping track of operations performed on tensors*, then going backwards through those operations, calculating gradients along the way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "bd283a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f386ac24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34877c6",
   "metadata": {},
   "source": [
    "To make sure PyTorch **keeps track of operations on a tensor** (i.e., creating computational graph), you need to set `requires_grad = True` on a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7ad0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a66c1",
   "metadata": {},
   "source": [
    "Or, similarly, use `requires_grad_(True)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93e64383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1., 2., 3])\n",
    "y.requires_grad_(True)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0f229",
   "metadata": {},
   "source": [
    "Let's perform a simple operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09dded0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = 2*y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692f885",
   "metadata": {},
   "source": [
    "`z` has a `grad_fn` attribute that can be used to compute the gradients. In fact, each variable has this attribute, but if no operation is performed, it is set to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b30f3da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<MulBackward0 object at 0x0000023580EDE160>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf582c3",
   "metadata": {},
   "source": [
    "PyTorch uses `grad_fn` as a reference to the final operation node. Then, whenever backpropagation is performed, this attribute is used as the starting point to go backward.\n",
    "\n",
    "Let's compute the derivative of `z` w.r.t. `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7895147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# z.backward() # ERROR: grad can be implicitly created only for scalar outputs\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f69068ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "500c880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6667, 0.6667, 0.6667])\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323188d",
   "metadata": {},
   "source": [
    "## Prevent gradient computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80b0c8",
   "metadata": {},
   "source": [
    "There are various ways to turn off gradient computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "589a6bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb05f0",
   "metadata": {},
   "source": [
    "Way 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88cc54f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ec3a0",
   "metadata": {},
   "source": [
    "Way 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e6ee2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "y = x.detach() # returns a copy with requires_grad=False\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d43994",
   "metadata": {},
   "source": [
    "Way 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b940c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = 2 * x\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22463ad1",
   "metadata": {},
   "source": [
    "## By default, PyTorch accumulates the gradients\n",
    "\n",
    "### $\\text{output} = \\frac{1}{3}\\sum_{i = 1}^{3} y_i ==> \\frac{d(\\text{output})}{d{y_i}} = \\[1/3, 1/3, 1/3]\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ec09c5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([0.6667, 0.6667, 0.6667])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1.3333, 1.3333, 1.3333])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    model_output = y.mean()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(y.grad)\n",
    "    \n",
    "    # y.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9b63a",
   "metadata": {},
   "source": [
    "## Loss and Autograd together\n",
    "\n",
    "When we create a network with PyTorch, all of the parameters are initialized with `requires_grad = True`. \n",
    "\n",
    "This means that when we calculate the loss and call `loss.backward()`, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. \n",
    "\n",
    "Below you can see an example of calculating the gradients using a backwards pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0fce19",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "10aa254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "88a9f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this cell\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b564a",
   "metadata": {},
   "source": [
    "### build a model and compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "f3c4b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "scores = model(images)\n",
    "loss = criterion(scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "2868eb7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0020, -0.0020, -0.0020,  ..., -0.0020, -0.0020, -0.0020],\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        ...,\n",
      "        [-0.0007, -0.0007, -0.0007,  ..., -0.0007, -0.0007, -0.0007],\n",
      "        [ 0.0022,  0.0022,  0.0022,  ...,  0.0022,  0.0022,  0.0022],\n",
      "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f5f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
