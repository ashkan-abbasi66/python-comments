{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer Learning is a machine learning method where we **reuse a pre-trained model** as the starting point for a model **on a new task**.\n",
    "\n",
    "The [`torchvision.models`](https://pytorch.org/vision/stable/models.html) includes models which were trained for different tasks:\n",
    "- image classification\n",
    "- pixelwise semantic segmentation\n",
    "- object detection\n",
    "- instance segmentation\n",
    "- person keypoint detection\n",
    "- video classification\n",
    "- optical flow.\n",
    "\n",
    "----------------\n",
    "Example: \n",
    "```python\n",
    "import torchvision.models as models\n",
    "alexnet = models.alexnet() # constructs the model with random weights\n",
    "alexnet = models.alexnet(pretrained=True) \n",
    "```\n",
    "Image classification models were trained on ImageNet. Thus, the models expect:\n",
    "- Input images: 3-channel RGB images in range [0, 1] and with shape (3 x H x W); where H & W >= 224.\n",
    "- Images should be normalized with `\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`\n",
    "\n",
    "An example can be found [here](https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101).\n",
    "\n",
    "------------------\n",
    "\n",
    "Here we'll use transfer learning to train a network that can classify our cat and dog photos with near perfect accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashkan\\Anaconda3\\envs\\py38torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are `[0.485, 0.456, 0.406]` and the standard deviations are `[0.229, 0.224, 0.225]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'E:\\POSTDOC\\PYTHON_CODES\\DATASETS\\dogs-vs-cats-kaggle'\n",
    "train_dir = os.path.join(data_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),              # PIL image ===> NCHW [0,1]\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "valid_dataset = datasets.ImageFolder(train_dir, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset object size 25000\n",
      "Validation dataset object size 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Train dataset object size {len(train_dataset)}')\n",
    "print(f'Validation dataset object size {len(train_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "N_data = len(train_dataset)\n",
    "N_valid = int(0.2*N_data)\n",
    "N_train = int(N_data - N_valid)\n",
    "\n",
    "print(N_train)\n",
    "print(N_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)  # Sets the seed for generating random numbers.\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 12345\n",
    "\n",
    "set_seed(seed_value)\n",
    "trainset, _ = torch.utils.data.random_split(train_dataset, [N_train, N_valid])\n",
    "set_seed(seed_value)\n",
    "_, validset = torch.utils.data.random_split(valid_dataset, [N_train, N_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have a shorter training time, let's reduce the number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches with size 512 in the training set is 8\n",
      "number of batches with size 512 in the validation set is 2\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=SequentialSampler(range(4096)))\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, sampler= SequentialSampler(range(1024)))\n",
    "\n",
    "print(f'number of batches with size {batch_size} in the training set is {len(trainloader)}')\n",
    "print(f'number of batches with size {batch_size} in the validation set is {len(validloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case, the dataset attribute of `DataLoader`\\'s object cannot represent the number of samples used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader.dataset))\n",
    "print(len(validloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader.sampler))\n",
    "print(len(validloader.sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Instead of this relatively tricky procedure in order to split training data into train and validation subsets, you can separate data outside of PyTorch's environment. Then, just load them using `datasets.ImageFolder`. For example, download the dataset from http://files.fast.ai/data/dogscats.zip. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the loaded checkpoint will be saved somewhere like this `C:\\Users\\ashkan/.cache\\torch\\checkpoints\\densenet121-a639ec97.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has two main parts: the feature extractor and the classifier. \n",
    "\n",
    "<img src='assets/transfer_learning.png' width=600px>\n",
    "\n",
    "For the above model, we need to replace `(fc): Linear(in_features=512, out_features=1000, bias=True)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters. Thus, backpropagation won't go through them.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier part\n",
    "model.fc = nn.Linear(in_features=512, out_features=2, bias=True) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "model.to(my_device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(trainloader, model, criterion, optimizer, testloader = None):\n",
    "    \n",
    "    if testloader is not None:\n",
    "        steps = 0\n",
    "        print_every = 5 # Evaluate the model every 5 steps within each epoch.\n",
    "        running_loss = 0\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        images = images.to(my_device)\n",
    "        labels = labels.to(my_device)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Additional step to print out the intermediate results \n",
    "        #  before finishing one epoch.\n",
    "        if testloader is not None:\n",
    "            if steps % print_every == 0:\n",
    "                test_loss,test_acc = test_loop(testloader, model, criterion)\n",
    "\n",
    "                print(f\"---> Train loss: {running_loss/(print_every*trainloader.batch_size):.6f}.. \"\n",
    "                      f\"Test loss: {test_loss:.6f}.. \"\n",
    "                      f\"Test accuracy: {test_acc:.3f}\")\n",
    "                \n",
    "                running_loss = 0\n",
    "            \n",
    "#     train_loss = total_loss / len(trainloader.dataset)\n",
    "    train_loss = total_loss / len(trainloader.sampler)\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def test_loop(testloader, model, criterion):\n",
    "    tot_test_loss = 0\n",
    "    test_correct = 0  # Number of correct predictions on the test set\n",
    "\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        for images, labels in testloader:\n",
    "            \n",
    "            images = images.to(my_device)\n",
    "            labels = labels.to(my_device)\n",
    "            \n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            tot_test_loss += loss.item()\n",
    "\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            test_correct += equals.sum().item()\n",
    "    \n",
    "    # test_loss = tot_test_loss / len(testloader.dataset)\n",
    "    # test_acc = test_correct / len(testloader.dataset)\n",
    "    test_loss = tot_test_loss / len(testloader.sampler)\n",
    "    test_acc = test_correct / len(testloader.sampler)\n",
    "    \n",
    "    # set model back to train mode\n",
    "    model.train()\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\n",
      "---> Train loss: 0.000272.. Test loss: 0.000962.. Test accuracy: 0.820\n",
      "---> Train loss: 0.000212.. Test loss: 0.000746.. Test accuracy: 0.846\n",
      "---> Train loss: 0.000188.. Test loss: 0.000553.. Test accuracy: 0.936\n",
      "---> Train loss: 0.000155.. Test loss: 0.000478.. Test accuracy: 0.946\n",
      "---> Train loss: 0.000139.. Test loss: 0.000388.. Test accuracy: 0.950\n",
      "---> Train loss: 0.000122.. Test loss: 0.000344.. Test accuracy: 0.956\n",
      "---> Train loss: 0.000111.. Test loss: 0.000327.. Test accuracy: 0.954\n",
      "---> Train loss: 0.000102.. Test loss: 0.000294.. Test accuracy: 0.957\n",
      "End of Epoch: 1/2..  Training Loss: 0.000813..  Valid Loss: 0.000294..  Valid Accuracy: 0.957\n",
      "Epoch: 2/2\n",
      "---> Train loss: 0.000111.. Test loss: 0.000264.. Test accuracy: 0.957\n",
      "---> Train loss: 0.000096.. Test loss: 0.000252.. Test accuracy: 0.958\n",
      "---> Train loss: 0.000084.. Test loss: 0.000243.. Test accuracy: 0.958\n",
      "---> Train loss: 0.000091.. Test loss: 0.000236.. Test accuracy: 0.956\n",
      "---> Train loss: 0.000089.. Test loss: 0.000232.. Test accuracy: 0.955\n",
      "---> Train loss: 0.000080.. Test loss: 0.000226.. Test accuracy: 0.955\n",
      "---> Train loss: 0.000082.. Test loss: 0.000218.. Test accuracy: 0.960\n",
      "---> Train loss: 0.000069.. Test loss: 0.000213.. Test accuracy: 0.966\n",
      "End of Epoch: 2/2..  Training Loss: 0.000440..  Valid Loss: 0.000213..  Valid Accuracy: 0.966\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "    \n",
    "for e in range(epochs):\n",
    "    print(f'Epoch: {e+1}/{epochs}')\n",
    "    \n",
    "    train_loss = train_loop(trainloader, model, criterion, optimizer, validloader)\n",
    "    \n",
    "    valid_loss,valid_acc = test_loop(validloader, model, criterion)\n",
    "\n",
    "    # Keep track of losses at the completion of epoch\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(valid_loss)\n",
    "\n",
    "    print(\"End of Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "          \"Training Loss: {:.6f}.. \".format(train_loss),\n",
    "          \"Valid Loss: {:.6f}.. \".format(valid_loss),\n",
    "          \"Valid Accuracy: {:.3f}\".format(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your model on Kaggle's test set and create a submission file\n",
    "\n",
    "As you know, for this dataset, you do not have a separate test set with labels. You have test data inside a folder named `test1`. For these images, labels are not provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from PIL import Image\n",
    "\n",
    "class DataWithNoLabels(Dataset):\n",
    "    def __init__(self, file_list, dirpath , transform = None):\n",
    "        self.file_list = file_list\n",
    "        self.dir = dirpath\n",
    "        self.transform = transform\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.dir, self.file_list[idx])) # PIL image [0, 255] - H*W*3\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            img = img.numpy()\n",
    "            return img.astype('float32'), self.file_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.path.join(data_dir, 'test1')\n",
    "test_files = os.listdir(test_dir)\n",
    "\n",
    "print(len(test_files))\n",
    "\n",
    "testset = DataWithNoLabels(test_files, test_dir, transform = test_transforms)\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = DataLoader(testset, batch_size = 256, shuffle=False, \n",
    "                        sampler=SequentialSampler(range(1024))) # Don't use it in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "\n",
    "fn_list = []    # file names are stored in this list\n",
    "pred_list = []  # outputs\n",
    "\n",
    "for x, fn in testloader:  # A Pytorch tensor (x) contains all data in a batch and a list (fn) contains associated file names.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        x = x.to(my_device)\n",
    "        \n",
    "        output = model(x)\n",
    "        \n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        \n",
    "        fn_list += [n[:-4] for n in fn]        # remove \".jpg\" from file names\n",
    "        pred_list += [p.item() for p in pred]  # pred is a PyTorch tensor which contains a list \n",
    "\n",
    "model.train()\n",
    "\n",
    "submission = pd.DataFrame({\"id\":fn_list, \"label\":pred_list})\n",
    "submission.to_csv('preds_resnet18.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "print(len(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 3, 224, 224])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Split the train set into train and validation subsets, and use the validation set during training.\n",
    "2. Use features extracted by the first layer (layer 1) and apply a FC layer as a classifier. How would be the accuracy?\n",
    "3. Use features extracted by the third layer (layer 3) and apply a FC layer. How much the accuracy will change?\n",
    "4. Use `nn.NLLLoss()` instead of `nn.CrossEntropyLoss()` as the criterion. What changes should be made?\n",
    "5. Use other models such as `densenet121`:\n",
    "```python\n",
    "model = models.densenet121(pretrained=True)\n",
    "# freeze parameters\n",
    "# add classifier\n",
    "model.classifier = nn.Sequential(nn.Linear(1024, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(256, 2),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n",
    "criterion = nn.NLLLoss()\n",
    "```\n",
    "\n",
    "6. Use part of the training set for training the model.\n",
    "\n",
    "    You can use:\n",
    "\n",
    "```python\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "indices = np.random.randint(0, len(dataset), size=(500, 1))\n",
    "dataset_500 = data_utils.Subset(dataset, indices)\n",
    "    \n",
    "```\n",
    "\n",
    "    or:\n",
    "\n",
    "```python\n",
    "\n",
    "dataset_500 = torch.utils.data.Subset(dataset, np.random.choice(len(dataset), 500, replace=False))\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# testset = datasets.ImageFolder(test_dir, transform=test_transforms)  # Note: test set is not divided into separate folders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
