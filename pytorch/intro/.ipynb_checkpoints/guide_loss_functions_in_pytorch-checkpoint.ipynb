{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb64a36",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "NOTE: The usage of cross entropy loss in PyTorch for multiclass and binary classification is the main topic of this document.\n",
    "\n",
    "**In PyTorch, we usually pass the raw output of our network into the loss, not the output of the softmax function. This is because `nn.CrossEntropyLoss` internally computes Softmax and cross entropy loss simultaneusly.**\n",
    "\n",
    "The raw, unnormalized output of a network is usually called the *logits* or *scores*. \n",
    "\n",
    "Using logits is better than probabilities (or direct outputs of a Softmax function) because probabilities are often very close to zero or one, and  floating-point numbers can't accurately represent values near zero or one ([read more here](https://docs.python.org/3/tutorial/floatingpoint.html)). Therefore, it is better to avoid doing calculations with probabilities. One approach is to use **log-probabilities**. In PyTorch, `nn.CrossEntropyLoss` expects logits. **It internally computes log probabilities (or the logarithm of Softmax's outputs using `nn.LogSoftmax`), and use those values for computing the loss function (using `nn.NLLLoss()`)**.\n",
    "\n",
    "**Note:** In Tensorflow/Keras, we have `tf.keras.losses.CategoricalCrossentropy(from_logits=False)`. \n",
    "- When softmax is used in your model, use `from_logits=False`\n",
    "- When softmax is not applied in your model (i.e., your model outputs logits), use `from_logits=True`.\n",
    "\n",
    "The second option (`from_logits=True`), which combines the computation of softmax and loss, is more effective in terms of numerical stability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148b4f7",
   "metadata": {},
   "source": [
    "## `nn.CrossEntropyLoss`\n",
    "\n",
    "This function expects two inputs: 1. logits (Nxd), and 2. targets (N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4202147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7920, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Assume that we have 3 examples. \n",
    "# And, our network outputs 5 numbers (logits) for each example.\n",
    "output = torch.randn(3, 5, requires_grad=True)\n",
    "\n",
    "# Targets or labels are expected to be numeric\n",
    "# Each target value must be in {0, 1, 2, 3, 4} because the network outputs 5 logits.\n",
    "target = torch.tensor([1, 0, 4])\n",
    "\n",
    "print(nn.CrossEntropyLoss()(output, target)) # logits and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bc5a4-4ae7-4217-863f-b75a57cdf24e",
   "metadata": {},
   "source": [
    "Note that `grad_fn` is set to `<NllLossBackward0>`, meaning that `nn.NLLLoss()` was internally used for loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8dfa9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n",
      "torch.Size([3, 5])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(output.dtype)\n",
    "print(target.dtype)\n",
    "\n",
    "print(output.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c6951-6d41-424c-b229-655148cf59a6",
   "metadata": {},
   "source": [
    "## `nn.CrossEntropyLoss` Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65af38db-54c5-4eba-a292-9e0c0ffb094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e14881-47b2-4546-99a3-e95e7950799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader))\n",
    "print(len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324666d-d9c6-4044-bd29-4ee54976c716",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d61d702-44b2-46dc-bf5e-23fddb4e88d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9108810361862183\n",
      "Training loss: 0.8520315013249715\n",
      "Training loss: 0.5252435129801433\n",
      "Training loss: 0.4329351637363434\n",
      "Training loss: 0.39127474762598674\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10)) # No need to apply Softmax\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # By default, reduction = 'mean'. \n",
    "# Hence, loss.item() contains the loss of entire mini-batch, but divided by the batch size.\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*images.shape[0]\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486fc6a0-9272-4711-88ea-d8a3adb83a42",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc24f5a",
   "metadata": {},
   "source": [
    "## `nn.LogSoftmax` \n",
    "\n",
    "It performs `log_softmax(x) = ln( softmax(x) )`, and it often used in multi-class classification tasks to transform raw scores (logits) into **normalized log probabilities**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1729537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5991, -1.2887, -3.1615, -0.8946, -2.6426],\n",
      "        [-1.6974, -1.2098, -3.0073, -2.1276, -1.0497],\n",
      "        [-1.6188, -3.0033, -2.4904, -1.9149, -0.6501]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0.2021, 0.2756, 0.0424, 0.4088, 0.0712],\n",
      "        [0.1832, 0.2983, 0.0494, 0.1191, 0.3500],\n",
      "        [0.1981, 0.0496, 0.0829, 0.1474, 0.5220]], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "last_layer_outputs = torch.randn(3, 5, requires_grad=True)\n",
    "outputs = nn.LogSoftmax(dim=1)(last_layer_outputs) # m = 3 samples, each has n = 5 features\n",
    "# This is in the form of one-hot encoded outputs, but with log softmax\n",
    "print(outputs)\n",
    "# We can convert those scores into a proper one-hot encoded vectors\n",
    "print(torch.exp(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c562e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2121, grad_fn=<NllLossBackward0>)\n",
      "Probabilities: tensor([[0.2021, 0.2756, 0.0424, 0.4088, 0.0712],\n",
      "        [0.1832, 0.2983, 0.0494, 0.1191, 0.3500],\n",
      "        [0.1981, 0.0496, 0.0829, 0.1474, 0.5220]], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor([1, 0, 4]) # target values for each sample\n",
    "\n",
    "print(nn.NLLLoss()(outputs, target)) # logSoftmax and labels\n",
    "\n",
    "# You can get the probabilities using the exponential function:\n",
    "print(\"Probabilities:\", torch.exp(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3eef1",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "In fact, `nn.CrossEntropyLoss()` combines `nn.LogSoftmax()` (log(softmax(x))) and `nn.NLLLoss()` in one single class. Therefore, the output from the network that is passed into `nn.CrossEntropyLoss` needs to be the raw output of the network (called logits), not the output of the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f50b9",
   "metadata": {},
   "source": [
    "## Use `nn.LogSoftmax` & `nn.NLLLoss()` to define a model and train it\n",
    "\n",
    "An alternative to `nn.CrossEntropyLoss()` is to directly use these two functions. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d73d5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb769837",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0100469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48643ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainloader))\n",
    "print(len(trainloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3f0b1",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a65f6821",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9763235883076986\n",
      "Training loss: 0.9304784668286642\n",
      "Training loss: 0.5560508148034413\n",
      "Training loss: 0.4427611797332764\n",
      "Training loss: 0.39288225973447166\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1)) #### you can also use \"F.log_softmax\"\n",
    "\n",
    "criterion = nn.NLLLoss() # By default, reduction = 'mean'. \n",
    "# Hence, loss.item() contains the loss of entire mini-batch, but divided by the batch size.\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*images.shape[0]\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f05df-84a1-4313-bec5-6a94c7f19bca",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c002a-4338-40cb-83c3-6d9c58926fd4",
   "metadata": {},
   "source": [
    "## Using `nn.CrossEntropyLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabd7d26-ded4-4693-974e-be30dd6d324d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "# trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "\n",
    "\n",
    "class BinaryMNISTDataset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        self.mnist_dataset = datasets.MNIST(root, train=train, \n",
    "                                            transform=transform, \n",
    "                                            target_transform=target_transform, \n",
    "                                            download=download)\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.mnist_dataset[index]\n",
    "        if label == 0:\n",
    "            new_label = 1\n",
    "        else:\n",
    "            new_label = 0\n",
    "\n",
    "        return image, new_label\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = BinaryMNISTDataset(root=\"~/.pytorch/MNIST_data/\", train=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a161d3-f32d-40e1-bea3-0eb236d61514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3cb95f7-88f8-4c4d-a3b7-f07eeb44080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 2)) \n",
    "\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "# print(images.shape)                        # torch.Size([64, 1, 28, 28])\n",
    "# images = images.view(images.shape[0], -1)  # torch.Size([64, 784])\n",
    "# print(images.dtype)                        # torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54219b2-f1c2-425c-9809-4a385e5b5fb8",
   "metadata": {},
   "source": [
    "Note that the network must output 2 logits because `nn.CrossEntropyLoss` expects target values to be in {0, 1}. Thus, if your the network outputs only one value, there is no corresponding logit for a target class of 1, and you will face out of bound indices error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4643c0a-b5f2-423f-a992-f770be7e96ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.19014976968566577\n",
      "Training loss: 0.06397702364822229\n",
      "Training loss: 0.045741876451671125\n",
      "Training loss: 0.03884067624881864\n",
      "Training loss: 0.03537146703427037\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*images.shape[0]\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b854d363-bdc8-4bdc-a6eb-e72080ab9c77",
   "metadata": {},
   "source": [
    "## Using `nn.BCELoss()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be745fc0-ab78-41b7-ae43-124b28dda3db",
   "metadata": {},
   "source": [
    "If you want your network to have only one output (logit), use `torch.sigmoid()` and `nn.BCELoss()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6b2ca7d-3ba3-41f5-8c90-89f92c87a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.22231331993959397\n",
      "Training loss: 0.08463542868081923\n",
      "Training loss: 0.05626678491201148\n",
      "Training loss: 0.0457951580032223\n",
      "Training loss: 0.040296036641153186\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 1),\n",
    "                      nn.Sigmoid()) \n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        images = images.view(images.shape[0], -1)\n",
    "        labels = labels.view(labels.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        \n",
    "        loss = criterion(output.double(), labels.double()) # to avoid getting errors for data types\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*images.shape[0]\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58707205-a43d-40ba-931a-a24f7393dde4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
