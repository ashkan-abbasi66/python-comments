{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "To find a network's parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is straightforward to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.\n",
    "\n",
    "Training multilayer networks is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a NN\n",
    "\n",
    "**RECALL:**\n",
    "\n",
    "Mini-batch Gradient Descebt: [GD-variants.pptx]\n",
    "```python\n",
    "Initialize theta\n",
    "for epoch in range(N_epochs):\n",
    "    Shuffle(training_data)\n",
    "    for i in range(0,N,batch_size): # N: number of training data\n",
    "        batch = training_data[i:i+batch_size] \n",
    "        grad = evaluate_gradient(loss,theta,batch)\n",
    "        eta = learning_schedule(epoch)\n",
    "        theta = theta - eta*grad\n",
    "```\n",
    "\n",
    "\n",
    "> Exercise: Implement the training pass for a network. If you implemented it correctly, you should see the training loss drop with each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "model.to(my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # By default, reduction = 'mean'. \n",
    "# Hence, loss.item() contains the loss of entire mini-batch, but divided by the batch size.\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "\n",
    "If you use GPU (either using `model.to('cuda')` or `model.cuda()`), It is required to move the loaded data and labels to GPU.\n",
    "\n",
    "Example: \n",
    "\n",
    "Instead of `output = model(images)`, use `output = model(images.to('cuda'))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9329210791905722\n",
      "Training loss: 0.878707010682424\n",
      "Training loss: 0.5405897535641988\n",
      "Training loss: 0.43724528576533\n",
      "Training loss: 0.39004647526741026\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass, get our logits\n",
    "        output = model(images.to(my_device)) # output = model(images) CAUSES ERROR\n",
    "        \n",
    "        # Calculate the loss with the logits and the labels\n",
    "        loss = criterion(output, labels.to(my_device)) # loss = criterion(output, labels) CAUSES ERROR\n",
    "        \n",
    "        # backward pass through the operations that created loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take an update step\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()*images.shape[0]\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optimizer.zero_grad()`: When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7412, -2.7696, -0.6944,  0.7767,  1.0533,  0.0689, -3.6214,  5.9623,\n",
      "          0.1455,  4.9425]], device='cuda:0')\n",
      "tensor([[1.2021e-04, 1.1683e-04, 9.3068e-04, 4.0525e-03, 5.3437e-03, 1.9967e-03,\n",
      "         4.9849e-05, 7.2409e-01, 2.1557e-03, 2.6115e-01]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    scores = model(img.to(my_device))\n",
    "\n",
    "print(scores)\n",
    "print(nn.Softmax(dim=1)(scores))\n",
    "\n",
    "helper.view_classify(img.cpu().view(1, 28, 28), nn.Softmax(dim=1)(scores.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
