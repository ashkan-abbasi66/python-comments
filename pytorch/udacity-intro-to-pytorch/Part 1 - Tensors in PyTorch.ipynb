{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some facts about vectors, matrices, and tensors:\n",
    "- A vector is a collection of numbers. <=> array. \n",
    "    - E.g., vector containing the cost of each material used by a producer.\n",
    "- A matrix is a collection of vectors.<=> 2D array. <br> \n",
    "    - The amount of consuming a specific material by different producers can be collected into a vector.\n",
    "    - Matrix multiplication can be used to efficiently compute total cost of materials for each producer (No need to use a loop).\n",
    "\n",
    "- In the same way, a tensor is a collection of matrices. Thus, we can perform computations more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning.\n",
    "\n",
    "Tensor, in a programmer point of view, is a multidimensional array.\n",
    "\n",
    "But, PyTorch's provides additional features for its tensors. The ability to exploit GPU and compute derivatives are the main features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other materials:\n",
    "- What is CUDA and CuDNN? How to install them? How to install PyTorch / Tensorflow\n",
    "- Work with Google Colab\n",
    "- How to load files into colab.\n",
    "- How to work with images in Python.\n",
    "- Introduction to PyTorch's tensors\n",
    "- Load Tensors and computations into GPU\n",
    "- Set seed for random generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![software layers](./assets/software_layers_cuda_cudnn_gpu.png)\n",
    "\n",
    "DL frameworks do not need CUDA or cuDNN when they are only running on CPU. \n",
    "\n",
    "Both BLAS and Eigen are low-level C/C++ libraries for linear algebra (matrix) operations.\n",
    "\n",
    "Keras is above TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Torch tensors (with comparison to numpy arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(3,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4) # It is not initialized\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5062, 0.8469, 0.2588],\n",
      "        [0.2707, 0.4115, 0.6839]])\n"
     ]
    }
   ],
   "source": [
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loosly speaking*, we can say that PyTorch's tensors inherits from numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "(2, 2)\n",
      "int32 \n",
      "\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([2, 2])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m_np = np.array([[1, 2],[3, 4]])\n",
    "\n",
    "print(type(m_np))\n",
    "print(m_np)\n",
    "print(m_np.shape)\n",
    "print(m_np.dtype, '\\n')\n",
    "\n",
    "m = torch.tensor([[1, 2],[3, 4]])\n",
    "print(type(m))\n",
    "print(m)\n",
    "print(m.shape)\n",
    "print(m.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type conversion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 \n",
      "\n",
      "torch.float32 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m_np = m_np.astype(np.float32) # Copy of the array, cast to a specified type.\n",
    "print(m_np.dtype, '\\n')\n",
    "\n",
    "m = m.to(torch.float32) # <=> m.float(); Note: tensor.to(device) / tensor.to(dtype) \n",
    "print(m.dtype, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "m2 = torch.tensor([[1, 2],[3, 4]], dtype = torch.float32)\n",
    "print(m2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of Common Operators and Computations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3.]\n",
      " [3. 3.]]\n",
      "[[9. 9.]\n",
      " [9. 9.]]\n",
      "0.0 \n",
      "\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "t_np = (np.ones(shape=(2,2)) * 7 - 1) / 2\n",
    "print(t_np)\n",
    "print(t_np**2)\n",
    "print(np.std(t_np),'\\n')\n",
    "\n",
    "t = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "print(t)\n",
    "print(t**2)\n",
    "print(torch.std(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross:  [0. 0. 1.]\n",
      "dot:  2.0\n",
      "multiply:  [2. 0. 0.]\n",
      "*:  [2. 0. 0.]\n",
      "\n",
      "\n",
      "cross:  tensor([ 0.,  0., -1.])\n",
      "dot:  tensor(2.)\n",
      "mul:  tensor([2., 0., 0.])\n",
      "*:  tensor([2., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "u1 = np.array([1., 0., 0.])\n",
    "u2 = np.array([2., 1., 0.])\n",
    "print(\"cross: \", np.cross(u1, u2))\n",
    "print(\"dot: \", np.dot(u1,u2))       # print(np.matmul(u1, u2))\n",
    "print(\"multiply: \", np.multiply(u1,u2))  # element-wise multiplication\n",
    "print(\"*: \", u1*u2)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([2., 1., 0.])         # y unit vector\n",
    "\n",
    "print(\"cross: \", torch.cross(v2, v1)) #  the cross product of vectors\n",
    "print(\"dot: \", torch.dot(v1,v2))    # the dot product of two 1D tensors\n",
    "print(\"mul: \", torch.mul(v1, v2))   # element-wise multiplication\n",
    "print(\"*: \", v1*v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "[0.         0.69314718 1.09861229]\n",
      "6\n",
      "[ 2.71828183  7.3890561  20.08553692] \n",
      "\n",
      "torch.float32\n",
      "tensor([0.0000, 0.6931, 1.0986])\n",
      "tensor(6.)\n",
      "tensor([ 2.7183,  7.3891, 20.0855])\n"
     ]
    }
   ],
   "source": [
    "u1 = np.array([1, 2, 3])\n",
    "print(u1.dtype)\n",
    "print(np.log(u1))\n",
    "print(np.sum(u1))\n",
    "print(np.exp(u1), '\\n')\n",
    "\n",
    "v1 = torch.tensor([1, 2, 3], dtype=torch.float32) # Note: remove dtype and see the effect.\n",
    "print(v1.dtype)\n",
    "print(torch.log(v1))\n",
    "print(torch.sum(v1))\n",
    "print(torch.exp(v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy to Torch and back\n",
    "\n",
    "To create a tensor from a Numpy array, use `torch.from_numpy()`. \n",
    "\n",
    "To convert a tensor to a Numpy array, use the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14275185 0.35578955 0.18266546]\n",
      " [0.78889028 0.04484153 0.79690825]]\n",
      "tensor([[0.1428, 0.3558, 0.1827],\n",
      "        [0.7889, 0.0448, 0.7969]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(2,3)\n",
    "b = torch.from_numpy(a)  # <=> torch.tensor(a)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the underlying numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "[[0.14275185 0.35578955 0.18266546]\n",
      " [0.78889028 0.04484153 0.79690825]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(b))\n",
    "print(b.numpy())\n",
    "print(type(b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2855, 0.7116, 0.3653],\n",
      "        [1.5778, 0.0897, 1.5938]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "b.mul_(2)  # In-place version of mul(); Note: torch.mul(b,2) or b.mul(2) return a new tensor\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does numpy array (a) matches (b)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28550371 0.7115791  0.36533093]\n",
      " [1.57778057 0.08968306 1.59381649]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor to Python's built-in numeric data\n",
    "\n",
    "Use `item()` to get a Python number from a tensor containing a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n",
      "2.5 \n",
      "\n",
      "tensor([[1]])\n",
      "1 \n",
      "\n",
      "tensor([10], dtype=torch.int32)\n",
      "10 \n",
      "\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = torch.tensor(2.5)\n",
    "print(x)\n",
    "print(x.item(),'\\n')\n",
    "\n",
    "x = torch.tensor([[1]])\n",
    "print(x)\n",
    "print(x.item(),'\\n')\n",
    "\n",
    "x = torch.tensor(np.array([10]))\n",
    "print(x)\n",
    "print(x.item(),'\\n')\n",
    "\n",
    "print(np.array([[10]]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute $y=\\sigma(w^Tx + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the input vector is drawn from a Normal distribution. Similarly, initialize the weight vector and bias. Compute `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "\n",
    "# features = torch.randn((1, 5)) # x = a random vector. Each of its elements is drawn from Normal distribution.\n",
    "features = torch.tensor([1,2,3,4,5], dtype = torch.float32)\n",
    "\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9994]])\n",
      "tensor([[0.9994]])\n"
     ]
    }
   ],
   "source": [
    "y = activation(torch.sum(features * weights) + bias)\n",
    "print(y)\n",
    "y = activation((features * weights).sum() + bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9994]])\n"
     ]
    }
   ],
   "source": [
    "y = activation(torch.dot(features, weights)  + bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way is to use matrix multiplication\n",
    "\n",
    "Use [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) or [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) for matrix multiplication.\n",
    "\n",
    "`torch.mm`: only performs *matrix* multiplication. I.e., your inputs sizes should be $n \\times m$ and $m \\times p$. Moreover, this function does not support broadcasting.<br>\n",
    "`torch.matmul`: It is more general than `torch.mm` in a way that it can be used to apply the matrix multiplication on two *tensors*. Also, it also supports broadcasting.\n",
    "\n",
    "To change the shape use:\n",
    "- `weights.reshape(a, b)`: may return a copy or a view of the original tensor.\n",
    "- `weights.resize_(a, b)`: Resizes `self` tensor (here, `weights`) to the specified size.\n",
    "- `weights.view(a, b)`: will return a new tensor with the same data as `weights` with size `(a, b)`. The returned tensor will share the underling data with the original tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1., 1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "test = torch.ones(2,2)\n",
    "print(test)\n",
    "print(test.view(1,4)) \n",
    "print(test) # Note that using view does not change the underlaying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4., 4., 4.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Let's change the underlying data with an in-place multiplication\n",
    "print(test.view(1,4).mul_(4))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Tip:** \n",
    "To reshape, use `view`. And, to copy a tensor, use `clone`. [here](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)\n",
    "\n",
    "\n",
    "> **Exercise**: Calculate the output of our little network using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9994]])\n"
     ]
    }
   ],
   "source": [
    "y = activation(torch.mm(features.view(1,5), weights.view(5,1)) + bias) # mm and view are the best options from the above.\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute $\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Tensor can run on either CPU or GPU.\n",
    "\n",
    "One of the major advantages of PyTorch is its robust acceleration on CUDA-compatible Nvidia GPUs. (“CUDA” stands for Compute Unified Device Architecture, which is Nvidia’s platform for parallel computing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.device_count():  1\n",
      "torch.cuda.current_device():  0\n",
      "torch.cuda.get_device_name(0):  NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.device_count(): \", torch.cuda.device_count())        # Returns the number of GPUs available.\n",
    "print(\"torch.cuda.current_device(): \", torch.cuda.current_device())    # Returns the index of a currently selected device. \n",
    "print(\"torch.cuda.get_device_name(0): \", torch.cuda.get_device_name(0))  # Gets the name of a device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('We have a GPU!')\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.tensor([[1, 2],[3, 4]])\n",
    "print(x0.device)\n",
    "\n",
    "x1 = torch.tensor([[1, 2],[3, 4]], device=my_device)\n",
    "print(x1.device)\n",
    "\n",
    "x2 = torch.tensor([[1, 2],[3, 4]])\n",
    "x2 = x2.to(my_device)\n",
    "print(x2.device)\n",
    "\n",
    "x3 = torch.tensor([[1, 2],[3, 4]])\n",
    "x3 = x3.cuda()\n",
    "print(x3.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using PyTorch, create the required tensors and implement the following operations:**\n",
    "\n",
    "- I) $\\mathbf{A} = \n",
    "\\begin{bmatrix} \n",
    "1, 2\\\\\n",
    "3, 4\n",
    "\\end{bmatrix} + \\begin{bmatrix} \n",
    "1, 1\\\\\n",
    "2, 2\n",
    "\\end{bmatrix} + \\begin{bmatrix} \n",
    "10, 10\\\\\n",
    "20, 20\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- II) $b = \\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}.\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function which takes in two input matrices A and B. Then, it (1) computes the column sum of A, (2) computes the sum of all elements of B, and (3) multiplies (1) and (2).**\n",
    "\n",
    "Example:\n",
    "$\\begin{equation}\n",
    "  A = \\begin{bmatrix}\n",
    "  1 & 1 \\\\\n",
    "  1 & 1\n",
    "  \\end{bmatrix}\n",
    "  \\text{and }\n",
    "  B = \\begin{bmatrix}\n",
    "  1 & 2 & 3 \\\\\n",
    "  1 & 2 & 3\n",
    "  \\end{bmatrix}\n",
    "  \\text{ ==> }\n",
    "  \\text{Output} =  \\begin{bmatrix}\n",
    "  2 & 2\n",
    "  \\end{bmatrix} \\cdot 12 = \\begin{bmatrix}\n",
    "  24 & 24\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function which takes in an input matrix A, then the function flattens it and appends a column which contains the row-wise indices of the elements.**\n",
    "\n",
    "Example: \n",
    "$\\begin{equation}\n",
    "  C = \\begin{bmatrix}\n",
    "  2 & 3 \\\\\n",
    "  -1 & 10\n",
    "  \\end{bmatrix}\n",
    "==>\n",
    "  \\text{Output} = \\begin{bmatrix}\n",
    "  0 & 2 \\\\\n",
    "  1 & 3 \\\\\n",
    "  2 & -1 \\\\\n",
    "  3 & 10\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a function which computes $y=sin(x)cos(x)$ and its derivatives for the given input. Use PyTorch's `autograd` module for automatic differentiation.** \n",
    "\n",
    "Hint: The explicit derivative of this function is $\\frac{dy}{dx}=cos(2x)$. Check your result by computing $y$ and its derivative for $x = \\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(math.pi/4, requires_grad=True)\n",
    "y = torch.sin(x)*torch.cos(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.3711e-08, grad_fn=<CosBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(2*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have written the following function in pure python, but I got two different results when I have used the same input value which is wrapped into a PyTorch tensor.**\n",
    "\n",
    "- **1. What is the problem?**\n",
    "- **2. Compute the gradient of this function with respect to its input at x = 2**\n",
    "\n",
    "**Debug the code and modify it to get the appropriate results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 911.25\n"
     ]
    }
   ],
   "source": [
    "def my_func(x):\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            x = x + 1\n",
    "        elif i == 1:\n",
    "            x = x**2\n",
    "        elif i == 2:\n",
    "            x = x/2\n",
    "        elif i == 3:\n",
    "            x = x**3\n",
    "        else:\n",
    "            x = x*10\n",
    "    return x\n",
    "\n",
    "print('\\n',my_func(int(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(640, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2, dtype = torch.int)\n",
    "y = my_func(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1822.5000)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
