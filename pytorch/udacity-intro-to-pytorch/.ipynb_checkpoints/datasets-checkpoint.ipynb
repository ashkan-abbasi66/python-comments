{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf0473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794adb2f",
   "metadata": {},
   "source": [
    "# Datasets in PyTorch\n",
    "\n",
    "With the objective to decouple the codes of loading/preprocessing data and training on data, there are two data primitives in PyTorch:\n",
    "- [`torch.usils.data.Dataset`]():  it stores the samples and their corresponding labels.\n",
    "- `torch.utils.data.DataLoader`:  It wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "Pytorch has lots of pre-loaded datasets: You can find them here: [Image Datasets](https://pytorch.org/vision/stable/datasets.html), [Text Datasets](https://pytorch.org/text/stable/datasets.html), and [Audio Datasets](https://pytorch.org/audio/stable/datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755d7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST(root = '~/.pytorch/MNIST_data/', download=True, train=True, transform=transform) # target_transform\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13af8c5",
   "metadata": {},
   "source": [
    "`~/` means the user's home directory.\n",
    "\n",
    "For example:\n",
    "\n",
    "`~/.pytorch/MNIST_data/` ==> `C:\\Users\\ashkan/.pytorch/F_MNIST_data/`<br>\n",
    "`~/.keras/datasets` ==> `C:\\Users\\ashkan\\.keras\\datasets`<br>\n",
    "`~/Anaconda3` ==> `C:\\Users\\ashkan\\Anaconda3`<br>\n",
    "\n",
    "In Windows, only PowerShell recognizes this shortcut. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e34b05",
   "metadata": {},
   "source": [
    "### The content of `torch.utils.data.Dataset`'s object can be accessed by indexing\n",
    "\n",
    "All datasets are subclasses of `torch.utils.data.Dataset` i.e, they have `__getitem__` and `__len__` methods implemented. \n",
    "\n",
    "The `__getitem__` function loads and returns a sample from the dataset at the given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce98f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "image, label = trainset[0]\n",
    "print(image.shape)\n",
    "print(label)\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894d30f",
   "metadata": {},
   "source": [
    "### Wrap an iteratable around Dataset object\n",
    "\n",
    "`trainloader` wraps an iterator around `trainset`. So, we can make an iterator with `iter(trainloader)`. Later, we'll use this to loop through the dataset for training, like\n",
    "\n",
    "```python\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "```\n",
    "\n",
    "`torchvision.datasets.MNIST` is inherited from `torch.utils.data.Dataset`.\n",
    "\n",
    "In general, subclasses of `torch.utils.data.Dataset` can be passed to a `torch.utils.data.DataLoader` which can load multiple samples in parallel using `torch.multiprocessing` workers. \n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
    "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=args.nThreads)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "051d908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader) # iter(.): returns an iterator for the given object. \n",
    "# Note: The object should be iteratable.\n",
    "#       Most built-in containers in Python like: list, tuple, string etc. are iterables.\n",
    "\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1938074",
   "metadata": {},
   "source": [
    "### Transforms\n",
    "\n",
    "[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor) Convert a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to tensor (torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]) - SCALING\n",
    "\n",
    "[Normalize](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize) a tensor image with the given mean and standard deviation. This transform does not support PIL Image.\n",
    "\n",
    "`transforms.Normalize` computes the following for each channel: \n",
    "$$image = \\frac{image - \\mu}{\\sigma}$$\n",
    "\n",
    "`transforms.Normalize((0.5,), (0.5,))` ==> \n",
    "It will normalize the image in the range [-1,1]. For example, the minimum value 0 will be converted to (0-0.5)/0.5=-1, the maximum value of 1 will be converted to (1-0.5)/0.5=1. You perform it with the hope that the distribution becomes nearly standard normal distribution with mean of 0 and a variance and standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ba08d",
   "metadata": {},
   "source": [
    "This is what one of the images looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056d607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(28, 28)\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(images[1].numpy().shape) # unsqueeze\n",
    "print(images[1].numpy().squeeze().shape)\n",
    "print(np.expand_dims(images[1].numpy().squeeze(), axis = 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef01679",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHwCAYAAAC7cCafAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAABYlAAAWJQFJUiTwAAAcz0lEQVR4nO3dfaxtdXkn8O9TqTIlvRe5qTXEQVBUUioq0KJQETB1cEgtKBj+aCUUmxbJIFSnNfVlsHUSmkxE0Y6ampZWUqliS9spKCqoWKSklwpjfUEKt0gqIDKA8lrwN3/sde3t6Tn3Ze99zzrntz+fZGfdvdZ69u9xucL3rL3XS7XWAgD040fGbgAAmC/hDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCd2WPsBnaHqrotyYYkW0ZuBQCmtX+SB1prB+xqYZfhnkmw7zO8AGCh9Pq1/JaxGwCAOdgyTdGo4V5Vz6iqP6yqf6mqR6tqS1W9p6qeOmZfALCejfa1fFU9O8m1SZ6W5C+TfD3JzyZ5Y5Ljq+qo1tp3x+oPANarMY/c/3cmwX52a+3E1tpbWmvHJbkgyfOS/M8RewOAdataa6s/aNWzkvxTJr8lPLu19oNtlv14km8nqSRPa609OMXnb05y6Hy6BYDR3NBaO2xXi8b6Wv64YXrltsGeJK2171XV3yZ5RZIXJ/nsSh8yhPhyDppLlwCwDo31tfzzhunNKyz/5jB97ir0AgBdGevIfeMwvX+F5Vvn7729D1npqwpfywOwyNbqde41TFf/hAAAWOfGCvetR+YbV1i+Ycl6AMBOGivcvzFMV/pN/TnDdKXf5AGAFYwV7lcP01dU1b/rYbgU7qgkDye5brUbA4D1bpRwb639U5IrM3nizVlLFr8zyV5J/mSaa9wBYNGN+VS4N2Ry+9kLq+rlSb6W5Igkx2bydfxbR+wNANat0c6WH47eD09yUSah/qYkz05yYZKXuK88AExn1Oe5t9a+leT0MXsAgN6s1evcAYApCXcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6MweYzcA693pp58+U/273/3uqWs3btw409j33Xff1LV//ud/PtPYF1544Uz1N91000z10DNH7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ2p1trYPcxdVW1OcujYfbB+HH744VPXfvrTn55p7A0bNsxUv1498sgjM9XP8qjdj33sYzONDavohtbaYbta5MgdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADqzx9gNwFpwwQUXTF27cePGmcZ+9NFHp6694oorZhp7TCeeeOJM9X/8x388de2WLVtmGvv666+fqR52t9GO3KtqS1W1FV53jtUXAKx3Yx+535/kPcvM//4q9wEA3Rg73O9rrZ03cg8A0BUn1AFAZ8Y+cn9KVf1Skv2SPJjkpiRfaK09MW5bALB+jR3uT0/ykSXzbquq01trn99RcVVtXmHRQTN3BgDr1Jhfy/9RkpdnEvB7JXl+kg8l2T/JFVX1gvFaA4D1a7Qj99baO5fM+kqSX6+q7yd5U5Lzkpy0g884bLn5wxH9oXNoEwDWnbV4Qt0Hh+nRo3YBAOvUWgz3u4fpXqN2AQDr1FoM95cM01tH7QIA1qlRwr2qDq6qfZaZ/8wk7x/eXry6XQFAH8Y6oe6UJG+pqquT3Jbke0meneSEJHsmuTzJ/xqpNwBY18YK96uTPC/JizL5Gn6vJPcl+WIm171/pLXWRuoNANa1UcJ9uEHNDm9SAzvraU972kz1z3zmM6euffDBB2ca+1d+5Vemrv34xz8+09hjuu2222aq32+//aauPfXUU2cae/Pmle6ftWNPPOEGnOx+a/GEOgBgBsIdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM9VaG7uHuauqzUkOHbsPVs9BBx00U/0//uM/Tl170UUXzTT2GWecMVP9erVhw4aZ6m+88capa2d5FnySHHXUUVPXXnfddTONzcK5obV22K4WOXIHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDozB5jNwBrQVVNXXvttdfOsZPF8cADD8xUf/HFF09d+9a3vnWmsV/60pdOXeuRr6wGR+4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnPc6cLt95660z1H/rQh6auvfHGG2cam+mcf/75U9e+9rWvnWnsAw88cKZ62N0cuQNAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHTGI1/pwmOPPTZT/ZlnnjmnTlgtDz744NS1X/3qV2ca+6d+6qemrt1nn31mGvvee++dqZ7F4MgdADozl3CvqpOr6n1VdU1VPVBVraou3kHNkVV1eVXdW1UPVdVNVXVOVT1pHj0BwKKa19fyb0vygiTfT3JHkoO2t3JV/WKSTyR5JMmfJbk3yS8kuSDJUUlOmVNfALBw5vW1/LlJnptkQ5Lt/nhZVRuS/EGSJ5Ic01o7o7X235O8MMmXkpxcVafOqS8AWDhzCffW2tWttW+21tpOrH5ykp9Icklr7e+3+YxHMvkGINnBHwgAwMrGOKHuuGH6yWWWfSHJQ0mOrKqnrF5LANCPMS6Fe94wvXnpgtba41V1W5KDkzwryde290FVtXmFRdv9zR8AejbGkfvGYXr/Csu3zt9797cCAP1ZizexqWG6w9/vW2uHLfsBkyP6Q+fZFACsF2McuW89Mt+4wvINS9YDAHbBGOH+jWH63KULqmqPJAckeTzJravZFAD0Yoxwv2qYHr/MsqOT/FiSa1trj65eSwDQjzHC/dIk9yQ5taoO3zqzqvZM8q7h7QdG6AsAujCXE+qq6sQkJw5vnz5MX1JVFw3/vqe19uYkaa09UFW/mknIf66qLsnk9rOvyuQyuUszuSUtADCFeZ0t/8Ikpy2Z96zhlST/nOTNWxe01i6rqpcleWuS1yTZM8ktSX4jyYU7eac7AGAZcwn31tp5Sc7bxZq/TfJf5zE+wGo68sgjp6494ogjZhr7iiuumKmexeB57gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ2Z1/PcAdgJr371q2eq98hXdoYjdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojOe5w4xOOumkmepPOOGEqWu/9a1vzTT2Zz7zmalr77rrrpnGfv7znz9T/SOPPDJ17aZNm2Yau6pmqofdzZE7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZzzylS7su+++M9X/6Z/+6dS1Rx999Exjt9Zmqp/FO97xjqlr77///pnG3nvvvWeqH3O7jTk27AxH7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGeEOAJ0R7gDQGc9zpwt/9Vd/NVP9i170ojl1sjg2btw4dgvr0n777Td2CywAR+4A0Jm5hHtVnVxV76uqa6rqgapqVXXxCuvuPyxf6XXJPHoCgEU1r6/l35bkBUm+n+SOJAftRM2NSS5bZv5X5tQTACykeYX7uZmE+i1JXpbk6p2o+XJr7bw5jQ8ADOYS7q21H4Z5Vc3jIwGAKY15tvy+VfVrSTYl+W6SL7XWbtqVD6iqzSss2pmfBQCgS2OG+88Prx+qqs8lOa21dvsoHQFAB8YI94eS/G4mJ9PdOsw7JMl5SY5N8tmqemFr7cEdfVBr7bDl5g9H9IfOo1kAWG9W/Tr31trdrbV3tNZuaK3dN7y+kOQVSf4uyYFJXr/afQFAL9bMTWxaa48n+fDw9ugxewGA9WzNhPvgO8N0r1G7AIB1bK2F+4uH6a3bXQsAWNGqh3tVHVFVT15m/nGZ3AwnSZa9dS0AsGNzOVu+qk5McuLw9unD9CVVddHw73taa28e/v17SQ4eLnu7Y5h3SJLjhn+/vbV27Tz6AoBFNK9L4V6Y5LQl8541vJLkn5NsDfePJDkpyc8keWWSH01yV5KPJXl/a+2aOfUEAAupWmtj9zB3rnNfPHffffdM9Zs2bZq69s4775xp7Lvuumvq2o9+9KMzjT2m008/fab6Aw44YOraJz/5P/wyuG5ceOGFU9eee+65O16JteaGle7psj1r7YQ6AGBGwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOuORr3RhzEe+7rvvvjONPcsjXxfZscceO3XtZz7zmTl2srq+973vTV17yCGHzDT27bffPlM9U/HIVwBAuANAd4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRmj7EbgHmoqtHqn/Oc58w0tue5T+eEE06YunbW/WXLli1T1z7++OMzjT3L/vaGN7xhprHf8pa3zFTP6nHkDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BmPfKUL119//Uz1xx9//NS1Z5999kxj33HHHVPXzvLo0VntvffeM9W/7W1vm6n+rLPOmrq2tTbT2GeeeebUtdddd91MY5988slT137+85+faWzWD0fuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANCZmvW5xmtRVW1OcujYfbB6nvGMZ8xU/w//8A9T127atGmmsR999NGpaz/5yU/ONPYsfu7nfm6m+lm32yz/7fr4xz8+09ive93rpq597LHHZhqbhXNDa+2wXS2a+ci9qjZV1eur6i+q6paqeriq7q+qL1bVGVW17BhVdWRVXV5V91bVQ1V1U1WdU1VPmrUnAFhke8zhM05J8oEk305ydZLbk/xkklcn+XCSV1bVKW2bP7Or6heTfCLJI0n+LMm9SX4hyQVJjho+EwCYwjzC/eYkr0ryN621H2ydWVW/neT6JK/JJOg/MczfkOQPkjyR5JjW2t8P89+e5KokJ1fVqa21S+bQGwAsnJm/lm+tXdVa++ttg32Yf2eSDw5vj9lm0clJfiLJJVuDfVj/kSRvG96eOWtfALCodvfZ8v86TB/fZt5xw3S5M4G+kOShJEdW1VN2Z2MA0Kt5fC2/rKraI8nWU0q3DfLnDdObl9a01h6vqtuSHJzkWUm+toMxNq+w6KBd6xYA+rE7j9zPT/LTSS5vrX1qm/kbh+n9K9Rtnb/3buoLALq2W47cq+rsJG9K8vUkv7yr5cN0hxexrnTtn+vcAVhkcz9yr6qzkrw3yVeTHNtau3fJKluPzDdmeRuWrAcA7IK5hntVnZPk/Um+kkmw37nMat8Yps9dpn6PJAdkcgLerfPsDQAWxdzCvap+K5Ob0Hw5k2C/e4VVrxqmxy+z7OgkP5bk2tba9PfkBIAFNpdwH25Ac36SzUle3lq7ZzurX5rkniSnVtXh23zGnkneNbz9wDz6AoBFNPMJdVV1WpLfyeSOc9ckObuqlq62pbV2UZK01h6oql/NJOQ/V1WXZHL72VdlcpncpZnckhYAmMI8zpY/YJg+Kck5K6zz+SQXbX3TWrusql6W5K2Z3J52zyS3JPmNJBe2Hh9VBwCrxCNfIcnhhx++45VWcOWVV8409saNK1040rdlvuHbJZdddtnUtb/5m78509i33HLLTPWwC8Z55CsAsLYIdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM54njvM6MADD5yp/o1vfOPUtW94wxtmGntM559//kz173rXu6auffjhh2caG1aR57kDAMIdALoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADrjka8AsHZ55CsAINwBoDvCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDMzh3tVbaqq11fVX1TVLVX1cFXdX1VfrKozqupHlqy/f1W17bwumbUnAFhke8zhM05J8oEk305ydZLbk/xkklcn+XCSV1bVKa21tqTuxiSXLfN5X5lDTwCwsOYR7jcneVWSv2mt/WDrzKr67STXJ3lNJkH/iSV1X26tnTeH8QGAbcz8tXxr7arW2l9vG+zD/DuTfHB4e8ys4wAAO2ceR+7b86/D9PFllu1bVb+WZFOS7yb5Umvtpt3cDwB0b7eFe1XtkeR1w9tPLrPKzw+vbWs+l+S01trtOznG5hUWHbSTbQJAd3bnpXDnJ/npJJe31j61zfyHkvxuksOSPHV4vSyTk/GOSfLZqtprN/YFAF2r/3gS+xw+tOrsJO9N8vUkR7XW7t2Jmj2SfDHJEUnOaa29d4bxNyc5dNp6AFgjbmitHbarRXM/cq+qszIJ9q8mOXZngj1JWmuPZ3LpXJIcPe++AGBRzDXcq+qcJO/P5Fr1Y4cz5nfFd4apr+UBYEpzC/eq+q0kFyT5cibBfvcUH/PiYXrrvPoCgEUzl3CvqrdncgLd5iQvb63ds511j6iqJy8z/7gk5w5vL55HXwCwiGa+FK6qTkvyO0meSHJNkrOraulqW1prFw3//r0kBw+Xvd0xzDskyXHDv9/eWrt21r4AYFHN4zr3A4bpk5Kcs8I6n09y0fDvjyQ5KcnPJHllkh9NcleSjyV5f2vtmjn0BAALa7dcCjc2l8IB0Im1cSkcADAu4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANCZXsN9/7EbAIA52H+aoj3m3MRa8cAw3bLC8oOG6dd3fyvdsM2mY7tNx3bbdbbZdNbydts//5Znu6Raa/NtZR2oqs1J0lo7bOxe1gvbbDq223Rst11nm02n1+3W69fyALCwhDsAdEa4A0BnhDsAdEa4A0BnFvJseQDomSN3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOjMQoV7VT2jqv6wqv6lqh6tqi1V9Z6qeurYva1Fw/ZpK7zuHLu/MVXVyVX1vqq6pqoeGLbJxTuoObKqLq+qe6vqoaq6qarOqaonrVbfY9uV7VZV+29n/2tVdclq9z+GqtpUVa+vqr+oqluq6uGqur+qvlhVZ1TVsv8dX/T9bVe3W2/7W6/Pc/8PqurZSa5N8rQkf5nJs3t/NskbkxxfVUe11r47Yotr1f1J3rPM/O+vch9rzduSvCCT7XBH/u2Z0Muqql9M8okkjyT5syT3JvmFJBckOSrJKbuz2TVkl7bb4MYkly0z/yvza2tNOyXJB5J8O8nVSW5P8pNJXp3kw0leWVWntG3uSGZ/SzLFdhv0sb+11hbileRTSVqS/7Zk/ruH+R8cu8e19kqyJcmWsftYi68kxyZ5TpJKcsywD128wrobktyd5NEkh28zf89M/uBsSU4d+3/TGtxu+w/LLxq775G32XGZBPOPLJn/9EwCqyV5zTbz7W/Tbbeu9reF+Fq+qp6V5BWZhNXvL1n8P5I8mOSXq2qvVW6Ndaq1dnVr7Ztt+K/CDpyc5CeSXNJa+/ttPuORTI5kk+TM3dDmmrOL240krbWrWmt/3Vr7wZL5dyb54PD2mG0W2d8y1XbryqJ8LX/cML1ymf+jv1dVf5tJ+L84yWdXu7k17ilV9UtJ9svkj6CbknyhtfbEuG2tK1v3v08us+wLSR5KcmRVPaW19ujqtbVu7FtVv5ZkU5LvJvlSa+2mkXtaK/51mD6+zTz7244tt9226mJ/W5Rwf94wvXmF5d/MJNyfG+G+1NOTfGTJvNuq6vTW2ufHaGgdWnH/a609XlW3JTk4ybOSfG01G1snfn54/VBVfS7Jaa2120fpaA2oqj2SvG54u22Q29+2Yzvbbasu9reF+Fo+ycZhev8Ky7fO33v3t7Ku/FGSl2cS8HsleX6SD2Xy29QVVfWC8VpbV+x/03koye8mOSzJU4fXyzI5OeqYJJ9d8J/Szk/y00kub619apv59rftW2m7dbW/LUq470gNU78DbqO19s7hd6u7WmsPtda+0lr79UxOQvxPSc4bt8Nu2P+W0Vq7u7X2jtbaDa21+4bXFzL5lu3vkhyY5PXjdjmOqjo7yZsyuernl3e1fJgu3P62ve3W2/62KOG+9S/VjSss37BkPbZv68koR4/axfph/5uj1trjmVzKlCzgPlhVZyV5b5KvJjm2tXbvklXsb8vYie22rPW6vy1KuH9jmD53heXPGaYr/SbPv3f3MF03X1GNbMX9b/j974BMTuy5dTWbWue+M0wXah+sqnOSvD+Ta66PHc78Xsr+tsRObrftWXf726KE+9XD9BXL3JXoxzO5qcPDSa5b7cbWqZcM04X5j8OMrhqmxy+z7OgkP5bk2gU+c3kaLx6mC7MPVtVvZXITmi9nElB3r7Cq/W0bu7Ddtmfd7W8LEe6ttX9KcmUmJ4KdtWTxOzP5a+xPWmsPrnJra1ZVHVxV+ywz/5mZ/AWcJNu93So/dGmSe5KcWlWHb51ZVXsmedfw9gNjNLaWVdURVfXkZeYfl+Tc4e1C7INV9fZMTgTbnOTlrbV7trO6/W2wK9utt/2tFuVeEsvcfvZrSY7I5I5ZNyc5srn97A9V1XlJ3pLJtx63JflekmcnOSGTO11dnuSk1tpjY/U4pqo6McmJw9unJ/kvmfxVf80w757W2puXrH9pJrcDvSST24G+KpPLli5N8tpFuLHLrmy34fKjg5N8LpNb1SbJIfm367jf3lrbGlbdqqrTklyU5Ikk78vyv5Vvaa1dtE3NiVnw/W1Xt1t3+9vYt8hbzVeS/5zJ5V3fTvJYkn/O5ASLfcbuba29MrkE5KOZnFV6XyY3ffhOkk9nco1ojd3jyNvnvEzONl7ptWWZmqMy+aPo/2XyM9D/zeSI4Elj/+9Zi9styRlJ/k8md5b8fia3U709k3ulv3Ts/y1raJu1JJ+zv8223Xrb3xbmyB0AFsVC/OYOAItEuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHTm/wOHBempVp8lRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap=plt.cm.gray); # images[1,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94f108",
   "metadata": {},
   "source": [
    "# Custom datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ceed0b",
   "metadata": {},
   "source": [
    "```python\n",
    "class CustomImageDataset(torch.utils.nn.Dataset):\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    "    def __len__(self):\n",
    "        ...\n",
    "    def __getiterm__(self, index):\n",
    "        ...\n",
    "```\n",
    "\n",
    "Example: \n",
    "    \n",
    "- [Dataset class](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10977f3b",
   "metadata": {},
   "source": [
    "# More Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28376e6",
   "metadata": {},
   "source": [
    "- [DATASETS & DATALOADERS](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "- [TRANSFORMS](https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html#transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669afb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
