{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer Learning is a machine learning method where we **reuse a pre-trained model** as the starting point for a model **on a new task**.\n",
    "\n",
    "The [`torchvision.models`](https://pytorch.org/vision/stable/models.html) includes models which were trained for different tasks:\n",
    "- image classification\n",
    "- pixelwise semantic segmentation\n",
    "- object detection\n",
    "- instance segmentation\n",
    "- person keypoint detection\n",
    "- video classification\n",
    "- optical flow.\n",
    "\n",
    "----------------\n",
    "Example: \n",
    "```python\n",
    "import torchvision.models as models\n",
    "alexnet = models.alexnet() # constructs the model with random weights\n",
    "alexnet = models.alexnet(pretrained=True) \n",
    "```\n",
    "Image classification models were trained on ImageNet. Thus, the models expect:\n",
    "- Input images: 3-channel RGB images in range [0, 1] and with shape (3 x H x W); where H & W >= 224.\n",
    "- Images should be normalized with `\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`\n",
    "\n",
    "An example can be found [here](https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101).\n",
    "\n",
    "------------------\n",
    "\n",
    "Here we'll use transfer learning to train a network that can classify our cat and dog photos with near perfect accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are `[0.485, 0.456, 0.406]` and the standard deviations are `[0.229, 0.224, 0.225]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches with size 512 in the training set is 49\n"
     ]
    }
   ],
   "source": [
    "data_dir = r'E:\\POSTDOC\\PYTHON_CODES\\DATASETS\\dogs-vs-cats-kaggle'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test1')\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print(f'number of batches with size {batch_size} in the training set is {len(trainloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the loaded checkpoint will be saved somewhere like this `C:\\Users\\ashkan/.cache\\torch\\checkpoints\\densenet121-a639ec97.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has two main parts: the feature extractor and the classifier. \n",
    "\n",
    "<img src='assets/transfer_learning.png' width=600px>\n",
    "\n",
    "For the above model, we need to replace `(fc): Linear(in_features=512, out_features=1000, bias=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters. Thus, backpropagation won't go through them.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier part\n",
    "model.fc = nn.Linear(in_features=512, out_features=2, bias=True) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "model.to(my_device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(trainloader, model, criterion, optimizer, testloader = None):\n",
    "    \n",
    "    if testloader is not None:\n",
    "        steps = 0\n",
    "        print_every = 5 # Evaluate the model every 5 steps within each epoch.\n",
    "        running_loss = 0\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        images = images.to(my_device)\n",
    "        labels = labels.to(my_device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Additional step to print out the intermediate results \n",
    "        #  before finishing one epoch.\n",
    "        if testloader is not None:\n",
    "            if steps % print_every == 0:\n",
    "                test_loss,test_acc = test_loop(testloader, model, criterion)\n",
    "\n",
    "                print(f\"---> Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                      f\"Test loss: {test_loss:.3f}.. \"\n",
    "                      f\"Test accuracy: {test_acc:.3f}\")\n",
    "                \n",
    "                running_loss = 0\n",
    "            \n",
    "    train_loss = total_loss / len(trainloader.dataset)\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def test_loop(testloader, model, criterion):\n",
    "    tot_test_loss = 0\n",
    "    test_correct = 0  # Number of correct predictions on the test set\n",
    "\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        for images, labels in testloader:\n",
    "            \n",
    "            images = images.to(my_device)\n",
    "            labels = labels.to(my_device)\n",
    "            \n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            tot_test_loss += loss.item()\n",
    "\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            test_correct += equals.sum().item()\n",
    "    \n",
    "    test_loss = tot_test_loss / len(testloader.dataset)\n",
    "    test_acc = test_correct / len(testloader.dataset)\n",
    "    \n",
    "    # set model back to train mode\n",
    "    model.train()\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n",
      "---> Train loss: 0.183.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.022.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.004.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.001.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n",
      "---> Train loss: 0.000.. Test loss: 0.000.. Test accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "    \n",
    "for e in range(epochs):\n",
    "    print(f'Epoch: {e+1}/{epochs}')\n",
    "    \n",
    "    train_loss = train_loop(trainloader, model, criterion, optimizer, testloader)\n",
    "    \n",
    "    test_loss,test_acc = test_loop(testloader, model, criterion)\n",
    "\n",
    "    # Keep track of losses at the completion of epoch\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(\"End of Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "          \"Training Loss: {:.3f}.. \".format(train_loss),\n",
    "          \"Test Loss: {:.3f}.. \".format(test_loss),\n",
    "          \"Test Accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Split the train set into train and validation subsets, and use the validation set during training.\n",
    "2. Use features extracted by the first layer (layer 1) and apply a FC layer as a classifier. How would be the accuracy?\n",
    "3. Use features extracted by the third layer (layer 3) and apply a FC layer. How much the accuracy will change?\n",
    "4. Use `nn.NLLLoss()` instead of `nn.CrossEntropyLoss()` as the criterion. What changes should be made?\n",
    "5. Use other models such as `densenet121`:\n",
    "```python\n",
    "model = models.densenet121(pretrained=True)\n",
    "# freeze parameters\n",
    "# add classifier\n",
    "model.classifier = nn.Sequential(nn.Linear(1024, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(256, 2),\n",
    "                                 nn.LogSoftmax(dim=1))\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n",
    "criterion = nn.NLLLoss()\n",
    "```\n",
    "\n",
    "6. Use part of the training set for training the model.\n",
    "\n",
    "    You can use:\n",
    "\n",
    "```python\n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "\n",
    "indices = np.random.randint(0, len(dataset), size=(500, 1))\n",
    "dataset_500 = data_utils.Subset(dataset, indices)\n",
    "    \n",
    "```\n",
    "\n",
    "    or:\n",
    "\n",
    "```python\n",
    "\n",
    "dataset_500 = torch.utils.data.Subset(dataset, np.random.choice(len(dataset), 500, replace=False))\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
