{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "To find a network's parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is straightforward to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.\n",
    "\n",
    "Training multilayer networks is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/backprop_diagram.png' width=550px>\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial L_1}{\\partial W_1} \\frac{\\partial S}{\\partial L_1} \\frac{\\partial L_2}{\\partial S} \\frac{\\partial \\ell}{\\partial L_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "\\large W^\\prime_1 = W_1 - \\alpha \\frac{\\partial \\ell}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "The learning rate $\\alpha$ is set such that the weight update steps are small enough that the iterative method settles in a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a NN\n",
    "\n",
    "**RECALL:**\n",
    "\n",
    "Mini-batch Gradient Descebt: [GD-variants.pptx]\n",
    "```python\n",
    "Initialize theta\n",
    "for epoch in range(N_epochs):\n",
    "    Shuffle(training_data)\n",
    "    for i in range(0,N,batch_size):\n",
    "        batch = training_data[i:i+batch_size]\n",
    "        grad = evaluate_gradient(loss,theta,batch)\n",
    "        eta = learning_schedule(epoch)\n",
    "        theta = theta - eta*grad\n",
    "```\n",
    "\n",
    "\n",
    "> Exercise: Implement the training pass for a network. If you implemented it correctly, you should see the training loss drop with each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNIST datasets are hosted on yann.lecun.com that has moved under CloudFlare protection\n",
    "# Run this script to enable the datasets download\n",
    "# Reference: https://github.com/pytorch/vision/issues/1938\n",
    "\n",
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `~/` means the user's home directory.\n",
    "\n",
    "For example:\n",
    "\n",
    "`~/.pytorch/MNIST_data/` ==> `C:\\Users\\ashkan/.pytorch/F_MNIST_data/`<br>\n",
    "`~/.keras/datasets` ==> `C:\\Users\\ashkan\\.keras\\datasets`<br>\n",
    "`~/Anaconda3` ==> `C:\\Users\\ashkan\\Anaconda3`<br>\n",
    "\n",
    "In Windows, only PowerShell recognizes this shortcut. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a `torch.utils.data.Dataset`'s object's content can be accessed by indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "image, label = trainset[0]\n",
    "print(image.shape)\n",
    "print(label)\n",
    "print(len(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torchvision.datasets.MNIST` is inherited from `torch.utils.data.Dataset`.\n",
    "\n",
    "All datasets are subclasses of `torch.utils.data.Dataset` i.e, they have `__getitem__` and `__len__` methods implemented. \n",
    "\n",
    "Subclasses of `torch.utils.data.Dataset` can be passed to a `torch.utils.data.DataLoader` which can load multiple samples in parallel using `torch.multiprocessing` workers. \n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
    "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=args.nThreads)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8706892916896958\n",
      "Training loss: 0.8290717470556942\n",
      "Training loss: 0.5161565091850152\n",
      "Training loss: 0.4236779830762064\n",
      "Training loss: 0.3822302876282602\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass, get our logits\n",
    "        output = model(images) \n",
    "        \n",
    "        # Calculate the loss with the logits and the labels\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backward pass through the operations that created loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take an update step\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optimizer.zero_grad()`: When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.5693,  7.4522,  3.2158,  2.5338, -0.8993, -0.7080, -2.4576, -0.1703,\n",
      "          3.9846, -0.7124]])\n",
      "tensor([[2.8398e-07, 9.4849e-01, 1.3715e-02, 6.9340e-03, 2.2388e-04, 2.7107e-04,\n",
      "         4.7125e-05, 4.6411e-04, 2.9585e-02, 2.6988e-04]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    scores = model(img)\n",
    "\n",
    "print(scores)\n",
    "print(nn.Softmax(dim=1)(scores))\n",
    "\n",
    "helper.view_classify(img.view(1, 28, 28), nn.Softmax(dim=1)(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
